import argparse
import asyncio
import os
from time import time
from tqdm import tqdm
from crawler.async_crawler import AsyncCrawler
from crawler.config_loader import ConfigLoader
from storage.json_storage import JSONStorage
from storage.sqlite_storage import SQLiteStorage


async def main():
    parser = argparse.ArgumentParser(description="Advanced Async Web Crawler CLI")
    parser.add_argument("--urls", nargs="+", help="–°—Ç–∞—Ä—Ç–æ–≤—ã–µ URL –¥–ª—è –∫—Ä–∞—É–ª–∏–Ω–≥–∞")
    parser.add_argument("--max-pages", type=int, default=100, help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü")
    parser.add_argument("--max-depth", type=int, default=2, help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –∫—Ä–∞—É–ª–∏–Ω–≥–∞")
    parser.add_argument("--output", type=str, default="results.json", help="–§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    parser.add_argument("--config", type=str, help="–ü—É—Ç—å –∫ YAML/JSON –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
    parser.add_argument("--respect-robots", action="store_true", help="–°–æ–±–ª—é–¥–∞—Ç—å robots.txt")
    parser.add_argument("--rate-limit", type=float, default=1.0, help="–õ–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É")
    parser.add_argument("--max-concurrent", type=int, default=5, help="–ú–∞–∫—Å–∏–º—É–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á")

    args = parser.parse_args()

    # --- –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ ---
    if args.config:
        config_loader = ConfigLoader(args.config)
        config = config_loader.config
        start_urls = config.get("start_urls", [])
        max_pages = config.get("max_pages", args.max_pages)
        max_depth = config.get("max_depth", args.max_depth)
        rate_limit = config.get("rate_limit", args.rate_limit)
        max_concurrent = config.get("max_concurrent", args.max_concurrent)
        respect_robots = config.get("respect_robots", args.respect_robots)
        storage_config = config.get("storage", {"type": "json", "path": args.output})
    else:
        start_urls = args.urls or []
        max_pages = args.max_pages
        max_depth = args.max_depth
        rate_limit = args.rate_limit
        max_concurrent = args.max_concurrent
        respect_robots = args.respect_robots
        storage_config = {"type": "json", "path": args.output}

    if not start_urls:
        print("‚ùå –ù–µ —É–∫–∞–∑–∞–Ω—ã —Å—Ç–∞—Ä—Ç–æ–≤—ã–µ URL. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --urls –∏–ª–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª.")
        return

    # --- –í—ã–±–∏—Ä–∞–µ–º storage ---
    if storage_config["type"] == "json":
        storage = JSONStorage(storage_config["path"])
    elif storage_config["type"] == "sqlite":
        storage = SQLiteStorage(storage_config["path"])
    else:
        storage = None

    async with AsyncCrawler(
            max_concurrent=max_concurrent,
            max_depth=max_depth,
            respect_robots=respect_robots,
            requests_per_second=rate_limit,
            storage=storage
    ) as crawler:

        print("üöÄ –ó–∞–ø—É—Å–∫ –∫—Ä–∞—É–ª–∏–Ω–≥–∞...")

        # --- –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ ---
        start_time = time()
        progress_bar = tqdm(total=max_pages, desc="Pages Crawled", unit="page", dynamic_ncols=True)

        async def crawl_with_progress():
            results = []
            in_progress = set()

            # –ü–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            async def track_page(url):
                in_progress.add(url)
                page = await crawler._process_url(url)
                in_progress.remove(url)

                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
                progress_bar.update(1)
                elapsed = time() - start_time
                speed = progress_bar.n / elapsed if elapsed > 0 else 0
                remaining = max_pages - progress_bar.n
                eta = remaining / speed if speed > 0 else 0

                success_count = len([p for p in crawler.processed_urls.values() if p])
                failed_count = len(crawler.failed_urls)
                progress_bar.set_postfix({
                    "Speed": f"{speed:.2f} p/s",
                    "ETA": f"{int(eta)}s",
                    "Active Tasks": len(in_progress),
                    "Success": success_count,
                    "Failed": failed_count
                })
                return page

            # --- –û—Å–Ω–æ–≤–Ω–æ–π –∫—Ä–∞—É–ª–∏–Ω–≥ ---
            pages = await crawler.crawl(start_urls=start_urls, max_pages=max_pages)

            # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            for page in pages:
                await track_page(page["url"])

            return pages

        results = await crawl_with_progress()
        progress_bar.close()

        print(f"‚úÖ –ö—Ä–∞—É–ª–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(results)} —Å—Ç—Ä–∞–Ω–∏—Ü.")

        # --- –≠–∫—Å–ø–æ—Ä—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ ---
        crawler.stats_exporter.export_to_json("stats.json")
        crawler.stats_exporter.export_to_html_report("report.html")
        print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ HTML-–æ—Ç—á—ë—Ç —Å–æ–∑–¥–∞–Ω—ã: stats.json, report.html")

        # --- –ó–∞–∫—Ä—ã—Ç–∏–µ storage ---
        if crawler.storage:
            await crawler.storage.close()


if __name__ == "__main__":
    asyncio.run(main())
