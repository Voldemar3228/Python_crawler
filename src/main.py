import asyncio
from crawler.async_crawler import AsyncCrawler
from crawler.config_loader import ConfigLoader
from storage.json_storage import JSONStorage
from storage.csv_storage import CSVStorage
from storage.sqlite_storage import SQLiteStorage


async def main():
    # üîπ –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = ConfigLoader("config.yaml")
    crawler_settings = config.get_crawler_settings()
    start_urls = config.get_start_urls()
    filters = config.get_filters()
    storage_config = config.get_storage_settings()

    # üîπ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
    storages = []

    if storage_config.get("json", {}).get("enabled"):
        s = storage_config["json"]
        storages.append(JSONStorage(s["filename"], batch_size=s.get("batch_size", 50)))

    if storage_config.get("csv", {}).get("enabled"):
        s = storage_config["csv"]
        storages.append(CSVStorage(s["filename"], delimiter=s.get("delimiter", ",")))

    if storage_config.get("sqlite", {}).get("enabled"):
        s = storage_config["sqlite"]
        sqlite_store = SQLiteStorage(s["db_path"], batch_size=s.get("batch_size", 50))
        await sqlite_store.init_db()
        storages.append(sqlite_store)

    # üîπ –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤ –æ–¥–∏–Ω –æ–±—ä–µ–∫—Ç (–ø—Ä–∏–º–µ—Ä: –∏—Å–ø–æ–ª—å–∑—É–µ–º JSONStorage, –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å MultiStorage)
    # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    storage = storages[0] if storages else None

    # üîπ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∞—É–ª–µ—Ä–∞
    crawler = AsyncCrawler(
        max_concurrent=crawler_settings.get("max_concurrent", 5),
        max_depth=crawler_settings.get("max_depth", 2),
        include_patterns=filters.get("include_patterns"),
        exclude_patterns=filters.get("exclude_patterns"),
        requests_per_second=crawler_settings.get("requests_per_second", 1.0),
        respect_robots=crawler_settings.get("respect_robots", True),
        user_agent=crawler_settings.get("user_agent", "AdvancedCrawler/1.0"),
        storage=storage
    )

    # üîπ –ö—Ä–∞—É–ª–∏–Ω–≥
    async with crawler:
        await crawler.crawl(start_urls, max_pages=crawler_settings.get("max_pages", 100))

        # üîπ –≠–∫—Å–ø–æ—Ä—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        crawler.stats_exporter.export_to_json("stats.json")
        crawler.stats_exporter.export_to_html_report("report.html")

        # üîπ –ó–∞–∫—Ä—ã—Ç–∏–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        if storage:
            await storage.close()

asyncio.run(main())
