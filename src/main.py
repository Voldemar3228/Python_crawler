import asyncio
from src.crawler.async_crawler import AsyncCrawler
from crawler.logger import setup_crawler_logger
from utils import save_json, compute_page_stats, compute_overall_stats
import logging

logger = setup_crawler_logger(level=logging.INFO)

# --- –°–ø–∏—Å–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Ç–µ—Å—Ç–∞ ---
URLS = [
    "https://example.com",
    "https://www.python.org",
    "https://www.wikipedia.org",
]


# --- –û—Å–Ω–æ–≤–Ω–∞—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ---
async def main():
    crawler = AsyncCrawler(max_concurrent=5)

    logger.info("‚ñ∂Ô∏è Start fetching and parsing pages...")

    # 1Ô∏è‚É£ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    results = await crawler.fetch_urls(URLS)

    parsed_pages = []
    for url, html in results:
        if html.startswith(("HTTP ERROR", "TIMEOUT ERROR", "NETWORK ERROR", "UNEXPECTED ERROR")):
            logger.warning(f"‚ùå Skipping {url} due to fetch error")
            continue

        # 2Ô∏è‚É£ –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
        parsed = await crawler.fetch_and_parse(url)
        parsed_pages.append(parsed)

    # 3Ô∏è‚É£ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON
    save_json("parsed_pages.json", parsed_pages)
    logger.info("‚úÖ Parsed pages saved to parsed_pages.json")

    # 4Ô∏è‚É£ –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
    logger.info("üìä Individual page stats:")
    for page in parsed_pages:
        stats = compute_page_stats(page)
        logger.info(stats)

    # 5Ô∏è‚É£ –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    overall_stats = compute_overall_stats(parsed_pages)
    logger.info("üìà Overall stats:")
    logger.info(overall_stats)

    await crawler.close()


# --- –ó–∞–ø—É—Å–∫ ---
if __name__ == "__main__":
    asyncio.run(main())
