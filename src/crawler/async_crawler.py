# libraries
import aiohttp
import asyncio
import logging
import time
import re
import random
from urllib.parse import urljoin, urldefrag, urlparse
import async_timeout

from crawler.parser import HTMLParser
from crawler.logger import setup_crawler_logger
from crawler.semaphore_manager import SemaphoreManager
from crawler.queue import CrawlerQueue
from crawler.rate_limiter import RateLimiter
from crawler.robots_parser import RobotsParser
from crawler.retry_strategy import RetryStrategy
from crawler.errors import (
    TransientError,
    PermanentError,
    NetworkError,
    ParseError,
)
from crawler.circuit_breaker import CircuitBreaker


logger = setup_crawler_logger(level=logging.INFO)


class AsyncCrawler:
    def __init__(
        self,
        max_concurrent: int = 5,
        allowed_domains: list[str] | None = None,
        include_patterns: list[str] | None = None,
        exclude_patterns: list[str] | None = None,
        max_depth: int = 2,
        requests_per_second: float = 1.0,
        respect_robots: bool = True,
        min_delay: float = 0.0,
        jitter: float = 0.0,
        user_agent: str = "AsyncCrawler/1.0",
        timeout: aiohttp.ClientTimeout = None,
        connect_timeout=5,
        read_timeout=10,
        total_timeout=15
    ):
        self.max_concurrent = max_concurrent
        self.max_depth = max_depth

        # --- URL filters ---
        self.include_patterns = include_patterns or []
        self.exclude_patterns = exclude_patterns or []

        # --- Crawler state ---
        self.visited_urls: set[str] = set()
        self.failed_urls: dict[str, str] = {}
        self.processed_urls: dict[str, dict] = {}
        self.blocked_urls_by_robots: set[str] = set()
        self.request_times: list[float] = []

        # --- Semaphore / concurrency ---
        self.semaphore_manager = SemaphoreManager(global_limit=20, per_domain_limit=5)

        # # --- Timeout & session ---
        # if timeout is None:
        #     timeout = aiohttp.ClientTimeout(connect=5, sock_read=10)
        # connector = aiohttp.TCPConnector(limit=100, limit_per_host=10, keepalive_timeout=30)
        # self.session = aiohttp.ClientSession(timeout=timeout, connector=connector)

        # --- Parser ---
        self.parser = HTMLParser()

        # --- Rate limiter ---
        self.rate_limiter = RateLimiter(
            requests_per_second=requests_per_second,
            per_domain=True,
            min_delay=min_delay,
            jitter=jitter,
        )

        # --- Robots.txt ---
        self.robots_parser = RobotsParser()
        self.respect_robots = respect_robots
        self.user_agent = user_agent

        # --- Allowed domains ---
        self.allowed_domains = allowed_domains

        def _on_retry(exc, attempt, exc_type):
            logger.warning(f"ðŸ” Retry {attempt} for {exc_type.__name__}: {exc}")

        # --- Retry strategy ---
        self.retry_strategy = RetryStrategy(
            strategy={
                TransientError: {
                    "max_retries": 3,
                    "backoff_factor": 2.0,
                    "timeout_factor": 1.5  # ÐºÐ°Ð¶Ð´Ñ‹Ð¹ retry ÑƒÐ²ÐµÐ»Ð¸Ñ‡Ð¸Ð²Ð°ÐµÑ‚ Ñ‚Ð°Ð¹Ð¼Ð°ÑƒÑ‚ Ð½Ð° 50%
                },
                NetworkError: {
                    "max_retries": 2,
                    "backoff_factor": 1.5,
                    "timeout_factor": 1.2
                }
                # PermanentError Ð½Ðµ ÑƒÐºÐ°Ð·Ð°Ð½ â†’ Ð½Ðµ Ñ€ÐµÑ‚Ñ€Ð°Ð¸Ñ‚ÑÑ
            },
            on_retry=_on_retry,
        )

        # stats
        self.stats = {
            "errors": {},  # ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¾ÑˆÐ¸Ð±Ð¾Ðº Ð¿Ð¾ Ñ‚Ð¸Ð¿Ð°Ð¼, Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€: {"TransientError": 3}
            "success_retries": 0,  # ÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑƒÑÐ¿ÐµÑˆÐ½Ñ‹Ñ… Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð¾Ð² Ð±Ñ‹Ð»Ð¾
            "retry_times": [],  # Ð²Ñ€ÐµÐ¼Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ retry
            "permanent_failed_urls": {}  # ÑÐ¿Ð¸ÑÐ¾Ðº URL Ñ PermanentError
        }

        # Timeouts logic
        self.connect_timeout = connect_timeout
        self.read_timeout = read_timeout
        self.total_timeout = total_timeout
        self.session = None  # aiohttp session ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ Ð² __aenter__

        # for CircuitBreaker
        self.circuit_breaker = CircuitBreaker(
            max_errors=5,
            window=60.0,
            reset_timeout=30.0
        )

    # async def _do_request(self, url: str) -> str:
    async def _do_request(self, url: str, **kwargs) -> str:
        """
        Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ HTTP GET Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¾Ð¹ transient/permanent Ð¾ÑˆÐ¸Ð±Ð¾Ðº.
        Ð£ÑÑ‚Ð¾Ð¹Ñ‡Ð¸Ð²Ð¾ Ðº Ñ€Ð°Ð·Ñ€Ñ‹Ð²Ð°Ð¼ ÑÐ¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ñ Ð¸ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°Ð¼ Ñ Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼.
        """
        if not self.session:
            raise RuntimeError("Session is not initialized. Use 'async with AsyncCrawler()'")

        headers = {"User-Agent": self.user_agent}
        start_req = time.time()
        timeout = self.total_timeout

        try:
            async with async_timeout.timeout(timeout):
                async with self.session.get(url, headers=headers) as response:
                    # --- ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð¿Ð¾ ÑÑ‚Ð°Ñ‚ÑƒÑÑƒ ---
                    if response.status in (429, 503):
                        raise TransientError(f"HTTP {response.status}", status=response.status)
                    if response.status == 500:
                        raise TransientError("HTTP 500 Server Error", status=500)
                    if response.status in (401, 403, 404):
                        raise PermanentError(f"HTTP {response.status}", status=response.status)

                    response.raise_for_status()

                    # --- Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾Ðµ Ñ‡Ñ‚ÐµÐ½Ð¸Ðµ Ñ‚ÐµÐ»Ð° ---
                    try:
                        content = await response.read()  # Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼ ÐºÐ°Ðº bytes
                        text = content.decode("utf-8", errors="replace")  # Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾Ðµ Ð´ÐµÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
                    except Exception as e:
                        raise TransientError(f"Failed to read/parse response: {e}") from e

                    self.request_times.append(time.time() - start_req)
                    logger.info(f"âœ… Success {response.status}: {url}")
                    return text

        except PermanentError:
            # Ñ„Ð¸ÐºÑÐ¸Ñ€ÑƒÐµÐ¼ PermanentError, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð¿Ñ€ÐµÐ²Ñ€Ð°Ñ‰Ð°Ñ‚ÑŒ Ð² TransientError
            raise

        except asyncio.TimeoutError as e:
            raise TransientError("Timeout") from e
        except aiohttp.ClientConnectorError as e:
            raise NetworkError("Connection error") from e
        except aiohttp.ServerDisconnectedError as e:
            # ÑÐµÑ€Ð²ÐµÑ€ Ñ€Ð°Ð·Ð¾Ñ€Ð²Ð°Ð» ÑÐ¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ
            raise TransientError("Server disconnected") from e
        except aiohttp.ClientError as e:
            raise TransientError(f"Client error: {e}") from e

    # async context manager
    async def __aenter__(self):
        timeout = aiohttp.ClientTimeout(
            total=self.total_timeout,
            connect=self.connect_timeout,
            sock_read=self.read_timeout
        )
        connector = aiohttp.TCPConnector(limit=100, limit_per_host=10, keepalive_timeout=30)
        self.session = aiohttp.ClientSession(timeout=timeout, connector=connector)
        return self

    async def __aexit__(self, exc_type, exc, tb):
        await self.close()

    # --- Domain filter ---
    def _is_allowed_domain(self, url: str) -> bool:
        if not self.allowed_domains:
            return True
        domain = urlparse(url).netloc
        return any(domain.endswith(a) for a in self.allowed_domains)

    # --- URL filter ---
    def _is_allowed_url(self, url: str) -> bool:
        if not self._is_allowed_domain(url):
            return False
        for pattern in self.exclude_patterns:
            if re.search(pattern, url):
                return False
        if self.include_patterns:
            return any(re.search(p, url) for p in self.include_patterns)
        return True

    # --- Fetch one page ---
    async def fetch_url(self, url: str) -> str:
        domain = urlparse(url).netloc

        # --- Circuit breaker ---
        if self.circuit_breaker.is_blocked(domain):
            remaining = self.circuit_breaker.get_remaining_block(domain)
            logger.warning(f"ðŸš« Domain {domain} is temporarily blocked ({remaining:.1f}s remaining)")
            self.failed_urls[url] = f"Blocked by circuit breaker ({remaining:.1f}s)"
            return ""

        # --- robots.txt + rate limiter ---
        crawl_delay = 0
        if self.respect_robots:
            await self.robots_parser.fetch_robots(domain)
            allowed = await self.robots_parser.can_fetch(url, self.user_agent)
            if not allowed:
                logger.info(f"ðŸš« Blocked by robots.txt: {url}")
                self.failed_urls[url] = "Blocked by robots.txt"
                self.blocked_urls_by_robots.add(url)
                return ""
            crawl_delay = await self.robots_parser.get_crawl_delay(self.user_agent) or 0

        await self.rate_limiter.acquire(domain)
        if crawl_delay > 0:
            await asyncio.sleep(crawl_delay)

        # --- Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ñ„Ð¸ÐºÑÐ°Ñ†Ð¸Ð¸ Ð¾ÑˆÐ¸Ð±Ð¾Ðº ---
        def record_error_stats(exc):
            name = type(exc).__name__
            self.stats["errors"][name] = self.stats["errors"].get(name, 0) + 1
            self.failed_urls[url] = str(exc)

        # --- Callback Ð´Ð»Ñ retry ---
        def on_retry(exc, attempt, exc_type, delay=None, url=url):
            name = exc_type.__name__
            self.stats["errors"][name] = self.stats["errors"].get(name, 0) + 1

            delay_str = f"{delay:.2f}s" if delay else "-"
            logger.warning(f"ðŸ·ï¸ {name} | ðŸ”— {url} | ðŸ”¢ Attempt {attempt} | â° Next try in {delay_str} | ðŸŽ¯ Retrying")

            if attempt > 1:
                self.stats["success_retries"] += 1
            if delay:
                self.stats["retry_times"].append(delay)

            self.failed_urls[url] = str(exc)

        self.retry_strategy.on_retry = on_retry

        # --- Semaphore + retry ---
        async with self.semaphore_manager.limit(url):
            try:
                result = await self.retry_strategy.execute_with_retry(self._do_request, url=url)
                logger.info(f"ðŸŽ¯ Success | ðŸ”— {url}")
                return result

            except PermanentError as e:
                record_error_stats(e)
                logger.error(f"ðŸš« Permanent failure | ðŸ”— {url} | Reason: {str(e)}")
                return ""

            except Exception as e:
                record_error_stats(e)
                logger.exception(f"âŒ Failed after retries {url}: {e}")
                self.circuit_breaker.record_error(domain)
                return ""

    # --- Parse HTML ---
    async def parse_html(self, url: str, html: str) -> dict:
        try:
            return await self.parser.parse_html(html, url)
        except Exception as e:
            logger.exception(f"Parse error for {url}")
            raise ParseError(str(e)) from e

    # --- Process one page ---
    async def _process_url(self, url: str):
        if url in self.visited_urls:
            return None
        self.visited_urls.add(url)

        html = await self.fetch_url(url)
        if not html:
            return None

        parsed = await self.parse_html(url, html)
        self.processed_urls[url] = parsed
        return parsed

    # --- Crawl engine ---
    async def crawl(self, start_urls: list[str], max_pages: int = 100, progress_interval: float = 2.0):
        queue = CrawlerQueue()
        results = []

        for url in start_urls:
            if self._is_allowed_url(url):
                await queue.add_url(url, 0)

        async def worker():
            nonlocal results
            while len(self.visited_urls) < max_pages:
                item = await queue.get_next()
                if not item:
                    break
                url, depth = item
                if url in self.visited_urls:
                    continue

                parsed = await self._process_url(url)
                if parsed:
                    results.append(parsed)

                    # --- Safe link traversal ---
                    for link in parsed.get("links", []):
                        if not isinstance(link, str) or not link.strip():
                            continue
                        # if isinstance(link, tuple):
                        #     link = link[0]
                        absolute = urljoin(url, link)
                        absolute, _ = urldefrag(absolute)
                        if self._is_allowed_url(absolute) and depth + 1 <= self.max_depth:
                            await queue.add_url(absolute, depth + 1)

        workers = [asyncio.create_task(worker()) for _ in range(self.max_concurrent)]
        progress_task = asyncio.create_task(self._progress_logger(queue, interval=progress_interval))

        try:
            await asyncio.gather(*workers)
        finally:
            await progress_task
            await self.close()

        return results

    # --- Progress logger ---
    async def _progress_logger(self, queue: CrawlerQueue, interval: float = 2.0):
        prev_count = 0
        while True:
            processed_count = len(self.processed_urls)
            failed_count = len(self.failed_urls)
            blocked_count = len(self.blocked_urls_by_robots)
            in_queue = queue._queue.qsize()

            # ÑÐºÐ¾Ñ€Ð¾ÑÑ‚ÑŒ Ð¸ ÑÑ€ÐµÐ´Ð½ÑÑ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ°
            speed = (processed_count - prev_count) / interval
            prev_count = processed_count
            avg_delay = sum(self.request_times) / len(self.request_times) if self.request_times else 0

            logger.info(
                f"ðŸ“„ Processed: {processed_count} | "
                f"â³ In queue: {in_queue} | "
                f"âŒ Failed: {failed_count} | "
                f"ðŸš« Blocked: {blocked_count} | "
                f"âš¡ï¸ Speed: {speed:.2f} pages/sec | "
                f"â±ï¸ Avg delay: {avg_delay:.2f}s"
            )

            # ÐµÑÐ»Ð¸ Ð¾Ñ‡ÐµÑ€ÐµÐ´ÑŒ Ð¿ÑƒÑÑ‚Ð° Ð˜ Ð²ÑÐµ Ð²Ð¾Ñ€ÐºÐµÑ€Ñ‹ Ð·Ð°ÐºÐ¾Ð½Ñ‡Ð¸Ð»Ð¸, Ñ‚Ð¾ Ð²Ñ‹Ñ…Ð¾Ð´Ð¸Ð¼
            if in_queue == 0:
                # Ð´Ð°Ñ‘Ð¼ Ð²Ñ€ÐµÐ¼Ñ Ð²Ð¾Ñ€ÐºÐµÑ€Ð°Ð¼ Ð¿Ñ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ URL
                await asyncio.sleep(interval)
                in_queue_after_sleep = queue._queue.qsize()
                if in_queue_after_sleep == 0:
                    break

            await asyncio.sleep(interval)

    # --- Close session ---
    async def close(self):
        if self.session and not self.session.closed:
            await self.session.close()
