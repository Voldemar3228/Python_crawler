# demo.py
import asyncio
import json
import csv
import aiosqlite
from datetime import datetime

from crawler.async_crawler import AsyncCrawler
from storage.json_storage import JSONStorage
from storage.csv_storage import CSVStorage
from storage.sqlite_storage import SQLiteStorage


async def demo():
    # -----------------------
    # 1ï¸âƒ£ Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ñ…Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ð°
    # -----------------------
    json_storage = JSONStorage("demo_results.json", batch_size=10)
    csv_storage = CSVStorage("demo_results.csv", batch_size=10)
    sqlite_storage = SQLiteStorage("demo_results.db", batch_size=10)
    await sqlite_storage.init_db()

    # -----------------------
    # 2ï¸âƒ£ ÐšÑ€Ð°ÑƒÐ»ÐµÑ€ Ñ Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¼ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸ÐµÐ¼
    # -----------------------
    async with AsyncCrawler(
        max_concurrent=3,
        max_depth=1,
        storage=None  # Ð±ÑƒÐ´ÐµÐ¼ ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÑ‚ÑŒ Ð²Ñ€ÑƒÑ‡Ð½ÑƒÑŽ Ð¿Ð¾ÑÐ»Ðµ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°
    ) as crawler:

        # Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð²Ð¾ Ð²ÑÐµ Ñ‚Ñ€Ð¸ Ñ…Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ð°
        async def save_all(data):
            await asyncio.gather(
                json_storage.save(data),
                csv_storage.save(data),
                sqlite_storage.save(data)
            )

        start_urls = ["https://example.com"]
        results = []

        # ÐžÐ±Ð¾Ñ€Ð°Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ _process_url, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ
        original_process = crawler._process_url

        async def _process_and_save(url):
            standardized = await original_process(url)
            if standardized:
                await save_all(standardized)
                results.append(standardized)
            return standardized

        crawler._process_url = _process_and_save  # Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾ Ð·Ð°Ð¼ÐµÐ½ÑÐµÐ¼ Ð¼ÐµÑ‚Ð¾Ð´

        # Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ ÐºÑ€Ð°ÑƒÐ»ÐµÑ€
        await crawler.crawl(start_urls, max_pages=5)

    # -----------------------
    # 3ï¸âƒ£ Ð—Ð°ÐºÑ€Ñ‹Ð²Ð°ÐµÐ¼ Ñ…Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ð°
    # -----------------------
    await asyncio.gather(
        json_storage.close(),
        csv_storage.close(),
        sqlite_storage.close()
    )

    # -----------------------
    # 4ï¸âƒ£ Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
    # -----------------------
    print("ðŸ”¹ Statistics:")
    print(f"Pages crawled: {len(results)}")
    print(f"JSON pages: {len(results)}")
    print(f"CSV pages: {len(results)}")
    print(f"SQLite pages: {len(results)}\n")

    # -----------------------
    # 5ï¸âƒ£ Ð§Ñ‚ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…
    # -----------------------
    # JSON
    print("Reading first 3 pages from JSON:")
    with open("demo_results.json", "r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            if i >= 3:
                break
            data = json.loads(line)
            print(f"{i+1}. {data['url']} - {data['title']}")

    # CSV
    print("\nReading first 3 pages from CSV:")
    with open("demo_results.csv", "r", encoding="utf-8", newline="") as f:
        reader = csv.DictReader(f)
        for i, row in enumerate(reader):
            if i >= 3:
                break
            print(f"{i+1}. {row['url']} - {row['title']}")

    # SQLite
    print("\nReading first 3 pages from SQLite:")
    async with aiosqlite.connect("demo_results.db") as db:
        async with db.execute("SELECT url, title FROM pages LIMIT 3") as cursor:
            i = 0
            async for row in cursor:
                i += 1
                print(f"{i}. {row[0]} - {row[1]}")


if __name__ == "__main__":
    asyncio.run(demo())
