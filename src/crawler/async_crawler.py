# libraries
import aiohttp
import asyncio
import logging
import time
import re
import random
from urllib.parse import urljoin, urldefrag, urlparse
import async_timeout
from datetime import datetime

from crawler.parser import HTMLParser
from crawler.logger import setup_crawler_logger
from crawler.semaphore_manager import SemaphoreManager
from crawler.queue import CrawlerQueue
from crawler.rate_limiter import RateLimiter
from crawler.robots_parser import RobotsParser
from crawler.retry_strategy import RetryStrategy
from crawler.errors import (
    TransientError,
    PermanentError,
    NetworkError,
    ParseError,
)
from crawler.circuit_breaker import CircuitBreaker
from storage.base import DataStorage

logger = setup_crawler_logger(level=logging.INFO)


class AsyncCrawler:
    def __init__(
            self,
            max_concurrent: int = 5,
            allowed_domains: list[str] | None = None,
            include_patterns: list[str] | None = None,
            exclude_patterns: list[str] | None = None,
            max_depth: int = 2,
            requests_per_second: float = 1.0,
            respect_robots: bool = True,
            min_delay: float = 0.0,
            jitter: float = 0.0,
            user_agent: str = "AsyncCrawler/1.0",
            timeout: aiohttp.ClientTimeout = None,
            connect_timeout=5,
            read_timeout=10,
            total_timeout=15,
            storage: DataStorage | None = None
    ):
        self.max_concurrent = max_concurrent
        self.max_depth = max_depth

        # --- URL filters ---
        self.include_patterns = include_patterns or []
        self.exclude_patterns = exclude_patterns or []

        # --- Crawler state ---
        self.visited_urls: set[str] = set()
        self.failed_urls: dict[str, str] = {}
        self.processed_urls: dict[str, dict] = {}
        self.blocked_urls_by_robots: set[str] = set()
        self.request_times: list[float] = []

        # --- Semaphore / concurrency ---
        self.semaphore_manager = SemaphoreManager(global_limit=20, per_domain_limit=5)

        # # --- Timeout & session ---
        # if timeout is None:
        #     timeout = aiohttp.ClientTimeout(connect=5, sock_read=10)
        # connector = aiohttp.TCPConnector(limit=100, limit_per_host=10, keepalive_timeout=30)
        # self.session = aiohttp.ClientSession(timeout=timeout, connector=connector)

        # --- Parser ---
        self.parser = HTMLParser()

        # --- Rate limiter ---
        self.rate_limiter = RateLimiter(
            requests_per_second=requests_per_second,
            per_domain=True,
            min_delay=min_delay,
            jitter=jitter,
        )

        self.storage = storage

        # --- Robots.txt ---
        self.robots_parser = RobotsParser()
        self.respect_robots = respect_robots
        self.user_agent = user_agent

        # --- Allowed domains ---
        self.allowed_domains = allowed_domains

        def _on_retry(exc, attempt, exc_type):
            logger.warning(f"üîÅ Retry {attempt} for {exc_type.__name__}: {exc}")

        # --- Retry strategy ---
        self.retry_strategy = RetryStrategy(
            strategy={
                TransientError: {
                    "max_retries": 3,
                    "backoff_factor": 2.0,
                    "timeout_factor": 1.5  # –∫–∞–∂–¥—ã–π retry —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —Ç–∞–π–º–∞—É—Ç –Ω–∞ 50%
                },
                NetworkError: {
                    "max_retries": 2,
                    "backoff_factor": 1.5,
                    "timeout_factor": 1.2
                }
                # PermanentError –Ω–µ —É–∫–∞–∑–∞–Ω ‚Üí –Ω–µ —Ä–µ—Ç—Ä–∞–∏—Ç—Å—è
            },
            on_retry=_on_retry,
        )

        # stats
        self.stats = {
            "errors": {},  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –ø–æ —Ç–∏–ø–∞–º, –Ω–∞–ø—Ä–∏–º–µ—Ä: {"TransientError": 3}
            "success_retries": 0,  # —Å–∫–æ–ª—å–∫–æ —É—Å–ø–µ—à–Ω—ã—Ö –ø–æ–≤—Ç–æ—Ä–æ–≤ –±—ã–ª–æ
            "retry_times": [],  # –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è retry
            "permanent_failed_urls": {}  # —Å–ø–∏—Å–æ–∫ URL —Å PermanentError
        }

        # Timeouts logic
        self.connect_timeout = connect_timeout
        self.read_timeout = read_timeout
        self.total_timeout = total_timeout
        self.session = None  # aiohttp session —Å–æ–∑–¥–∞—ë–º –≤ __aenter__

        # for CircuitBreaker
        self.circuit_breaker = CircuitBreaker(
            max_errors=5,
            window=60.0,
            reset_timeout=30.0
        )

    # async def _do_request(self, url: str) -> str:
    async def _do_request(self, url: str, **kwargs) -> str:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç HTTP GET —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π transient/permanent –æ—à–∏–±–æ–∫.
        –£—Å—Ç–æ–π—á–∏–≤–æ –∫ —Ä–∞–∑—Ä—ã–≤–∞–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –∏ –ø—Ä–æ–±–ª–µ–º–∞–º —Å —Ç–µ–∫—Å—Ç–æ–º.
        """
        if not self.session:
            raise RuntimeError("Session is not initialized. Use 'async with AsyncCrawler()'")

        headers = {"User-Agent": self.user_agent}
        start_req = time.time()
        timeout = self.total_timeout

        try:
            async with async_timeout.timeout(timeout):
                async with self.session.get(url, headers=headers) as response:
                    # --- –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ —Å—Ç–∞—Ç—É—Å—É ---
                    if response.status in (429, 503):
                        raise TransientError(f"HTTP {response.status}", status=response.status)
                    if response.status == 500:
                        raise TransientError("HTTP 500 Server Error", status=500)
                    if response.status in (401, 403, 404):
                        raise PermanentError(f"HTTP {response.status}", status=response.status)

                    response.raise_for_status()

                    # --- –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ —Ç–µ–ª–∞ ---
                    try:
                        content = await response.read()  # —á–∏—Ç–∞–µ–º –∫–∞–∫ bytes
                        text = content.decode("utf-8", errors="replace")  # –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
                    except Exception as e:
                        raise TransientError(f"Failed to read/parse response: {e}") from e

                    self.request_times.append(time.time() - start_req)
                    logger.info(f"‚úÖ Success {response.status}: {url}")
                    return text

        except PermanentError:
            # —Ñ–∏–∫—Å–∏—Ä—É–µ–º PermanentError, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—Ä–∞—â–∞—Ç—å –≤ TransientError
            raise

        except asyncio.TimeoutError as e:
            raise TransientError("Timeout") from e
        except aiohttp.ClientConnectorError as e:
            raise NetworkError("Connection error") from e
        except aiohttp.ServerDisconnectedError as e:
            # —Å–µ—Ä–≤–µ—Ä —Ä–∞–∑–æ—Ä–≤–∞–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
            raise TransientError("Server disconnected") from e
        except aiohttp.ClientError as e:
            raise TransientError(f"Client error: {e}") from e

    # async context manager
    async def __aenter__(self):
        timeout = aiohttp.ClientTimeout(
            total=self.total_timeout,
            connect=self.connect_timeout,
            sock_read=self.read_timeout
        )
        connector = aiohttp.TCPConnector(limit=100, limit_per_host=10, keepalive_timeout=30)
        self.session = aiohttp.ClientSession(timeout=timeout, connector=connector)
        return self

    async def __aexit__(self, exc_type, exc, tb):
        await self.close()

    # --- Domain filter ---
    def _is_allowed_domain(self, url: str) -> bool:
        if not self.allowed_domains:
            return True
        domain = urlparse(url).netloc
        return any(domain.endswith(a) for a in self.allowed_domains)

    # --- URL filter ---
    def _is_allowed_url(self, url: str) -> bool:
        if not self._is_allowed_domain(url):
            return False
        for pattern in self.exclude_patterns:
            if re.search(pattern, url):
                return False
        if self.include_patterns:
            return any(re.search(p, url) for p in self.include_patterns)
        return True

    # --- Fetch one page ---
    async def fetch_url(self, url: str) -> str:
        domain = urlparse(url).netloc

        # --- Circuit breaker ---
        if self.circuit_breaker.is_blocked(domain):
            remaining = self.circuit_breaker.get_remaining_block(domain)
            logger.warning(f"üö´ Domain {domain} is temporarily blocked ({remaining:.1f}s remaining)")
            self.failed_urls[url] = f"Blocked by circuit breaker ({remaining:.1f}s)"
            return ""

        # --- robots.txt + rate limiter ---
        crawl_delay = 0
        if self.respect_robots:
            await self.robots_parser.fetch_robots(domain)
            allowed = await self.robots_parser.can_fetch(url, self.user_agent)
            if not allowed:
                logger.info(f"üö´ Blocked by robots.txt: {url}")
                self.failed_urls[url] = "Blocked by robots.txt"
                self.blocked_urls_by_robots.add(url)
                return ""
            crawl_delay = await self.robots_parser.get_crawl_delay(self.user_agent) or 0

        await self.rate_limiter.acquire(domain)
        if crawl_delay > 0:
            await asyncio.sleep(crawl_delay)

        # --- —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ñ–∏–∫—Å–∞—Ü–∏–∏ –æ—à–∏–±–æ–∫ ---
        def record_error_stats(exc):
            name = type(exc).__name__
            self.stats["errors"][name] = self.stats["errors"].get(name, 0) + 1
            self.failed_urls[url] = str(exc)

        # --- Callback –¥–ª—è retry ---
        def on_retry(exc, attempt, exc_type, delay=None, url=url):
            name = exc_type.__name__
            self.stats["errors"][name] = self.stats["errors"].get(name, 0) + 1

            delay_str = f"{delay:.2f}s" if delay else "-"
            logger.warning(f"üè∑Ô∏è {name} | üîó {url} | üî¢ Attempt {attempt} | ‚è∞ Next try in {delay_str} | üéØ Retrying")

            if attempt > 1:
                self.stats["success_retries"] += 1
            if delay:
                self.stats["retry_times"].append(delay)

            self.failed_urls[url] = str(exc)

        # self.retry_strategy.on_retry = on_retry

        # --- Semaphore + retry ---
        async with self.semaphore_manager.limit(url):
            try:
                result = await self.retry_strategy.execute_with_retry(
                    self._do_request,
                    url=url,
                    on_retry=on_retry
                )

                logger.info(f"üéØ Success | üîó {url}")
                return result

            except PermanentError as e:
                record_error_stats(e)
                logger.error(f"üö´ Permanent failure | üîó {url} | Reason: {str(e)}")
                return ""

            except Exception as e:
                record_error_stats(e)
                logger.exception(f"‚ùå Failed after retries {url}: {e}")
                self.circuit_breaker.record_error(domain)
                return ""

    # --- Parse HTML ---
    async def parse_html(self, url: str, html: str) -> dict:
        try:
            return await self.parser.parse_html(html, url)
        except Exception as e:
            logger.exception(f"Parse error for {url}")
            raise ParseError(str(e)) from e

    # --- Process one page ---
    async def _process_url(self, url: str):
        if url in self.visited_urls:
            return None
        self.visited_urls.add(url)

        html = await self.fetch_url(url)
        if not html:
            return None

        parsed = await self.parse_html(url, html)

        # üîπ –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
        standardized = {
            "url": url,
            "title": parsed.get("title", ""),
            "text": parsed.get("text", ""),
            "links": parsed.get("links", []),
            "metadata": parsed.get("metadata", {}),
            "crawled_at": datetime.utcnow(),
            "status_code": parsed.get("status_code", 200),
            "content_type": parsed.get("content_type", "text/html")
        }

        self.processed_urls[url] = standardized
        # üîπ –î–æ–±–∞–≤–ª—è–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ retry
        if self.storage:
            await self._save_with_retry(standardized)

        return standardized

    async def _save_with_retry(self, data, retries=3, delay=1):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ storage —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö.
        """
        for attempt in range(1, retries + 1):
            try:
                await self.storage.save(data)
                return
            except Exception as e:
                logger.warning(f"Save attempt {attempt} failed for {data['url']}: {e}")
                if attempt < retries:
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"Failed to save after {retries} attempts: {data['url']}")

    # --- Crawl engine ---
    async def crawl(self, start_urls: list[str], max_pages: int = 100, progress_interval: float = 2.0):
        queue = CrawlerQueue()
        results = []

        for url in start_urls:
            if self._is_allowed_url(url):
                await queue.add_url(url, 0)

        async def worker():
            nonlocal results

            while True:
                url, depth = await queue.get_next()

                try:
                    if url in self.visited_urls:
                        continue

                    parsed = await self._process_url(url)
                    if parsed:
                        results.append(parsed)

                        for link in parsed.get("links", []):
                            if not isinstance(link, str) or not link.strip():
                                continue

                            absolute = urljoin(url, link)
                            absolute, _ = urldefrag(absolute)

                            if (
                                    self._is_allowed_url(absolute)
                                    and depth + 1 <= self.max_depth
                                    and len(self.visited_urls) < max_pages
                            ):
                                await queue.add_url(absolute, depth + 1)

                finally:
                    queue.task_done()

        workers = [asyncio.create_task(worker()) for _ in range(self.max_concurrent)]
        progress_task = asyncio.create_task(self._progress_logger(queue, interval=progress_interval))

        try:
            await queue.join()
        finally:
            for w in workers:
                w.cancel()

            await asyncio.gather(*workers, return_exceptions=True)
            await progress_task

        return results

    # --- Progress logger ---
    async def _progress_logger(self, queue: CrawlerQueue, interval: float = 2.0):
        prev_count = 0
        while True:
            processed_count = len(self.processed_urls)
            failed_count = len(self.failed_urls)
            blocked_count = len(self.blocked_urls_by_robots)
            in_queue = queue._queue.qsize()

            # —Å–∫–æ—Ä–æ—Å—Ç—å –∏ —Å—Ä–µ–¥–Ω—è—è –∑–∞–¥–µ—Ä–∂–∫–∞
            speed = (processed_count - prev_count) / interval
            prev_count = processed_count
            avg_delay = sum(self.request_times) / len(self.request_times) if self.request_times else 0

            logger.info(
                f"üìÑ Processed: {processed_count} | "
                f"‚è≥ In queue: {in_queue} | "
                f"‚ùå Failed: {failed_count} | "
                f"üö´ Blocked: {blocked_count} | "
                f"‚ö°Ô∏è Speed: {speed:.2f} pages/sec | "
                f"‚è±Ô∏è Avg delay: {avg_delay:.2f}s"
            )

            # –µ—Å–ª–∏ –æ—á–µ—Ä–µ–¥—å –ø—É—Å—Ç–∞ –ò –≤—Å–µ –≤–æ—Ä–∫–µ—Ä—ã –∑–∞–∫–æ–Ω—á–∏–ª–∏, —Ç–æ –≤—ã—Ö–æ–¥–∏–º
            if in_queue == 0:
                # –¥–∞—ë–º –≤—Ä–µ–º—è –≤–æ—Ä–∫–µ—Ä–∞–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ URL
                await asyncio.sleep(interval)
                in_queue_after_sleep = queue._queue.qsize()
                if in_queue_after_sleep == 0:
                    break

            await asyncio.sleep(interval)

    # --- Close session ---
    async def close(self):
        if self.session and not self.session.closed:
            await self.session.close()

        # üîπ –ó–∞–∫—Ä—ã—Ç–∏–µ storage
        if self.storage:
            try:
                await self.storage.close()
            except Exception as e:
                logger.error(f"Failed to close storage: {e}")
