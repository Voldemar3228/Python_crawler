
# Python Async/SYNC Web Crawler

**–ü—Ä–æ–µ–∫—Ç:** –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤–µ–±-–∫—Ä–∞—É–ª–µ—Ä –¥–ª—è –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Å–∞–π—Ç–æ–≤.  
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —á–µ—Ä–µ–∑ YAML/CLI.

---

## üîπ –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

- –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ä–µ–∂–∏–º—ã
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ **–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≥–ª—É–±–∏–Ω—ã** (max_depth)
- –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ **—á–∏—Å–ª—É –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É**
- –§–∏–ª—å—Ç—Ä—ã URL –ø–æ –¥–æ–º–µ–Ω—É –∏ —à–∞–±–ª–æ–Ω–∞–º (include/exclude)
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π **retry –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫**
- –°–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏ —ç–∫—Å–ø–æ—Ä—Ç:
  - JSON
  - HTML –æ—Ç—á—ë—Ç
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö:
  - JSON
  - SQLite

---

## üîπ –£—Å—Ç–∞–Ω–æ–≤–∫–∞

```bash
git clone <—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π>
cd <–ø—Ä–æ–µ–∫—Ç>
python -m venv venv
source venv/bin/activate  # Linux / Mac
venv\Scripts\activate     # Windows
pip install -r requirements.txt
````

---

## üîπ –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### 1. –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫—Ä–∞—É–ª–µ—Ä

```python
from benchmark.sync_crawler import crawl_sync

start_urls = ["https://www.wikipedia.org/"]
results, count = crawl_sync(start_urls, max_pages=10)
print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {count}")
for r in results:
    print(r)
```

### 2. –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫—Ä–∞—É–ª–µ—Ä

```python
import asyncio
from benchmark.async_crawler import crawl_async

async def main():
    results, count = await crawl_async(max_pages=10)
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {count}")
    for r in results:
        print(r)

asyncio.run(main())
```

### 3. AdvancedCrawler (—Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º)

```python
from advanced_crawler import AdvancedCrawler

crawler = AdvancedCrawler(config_path="config.yaml")
asyncio.run(crawler.run())
```

---

## üîπ API

### `crawl_sync(start_urls, max_pages=100)`

* **start_urls** ‚Äî —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö URL
* **max_pages** ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
* **–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** `(results: list[str], visited_count: int)`

---

### `crawl_async(max_pages)`

* **max_pages** ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
* **–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:** `(results: list[dict], count: int)`
* –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å-–ª–æ–≥ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É.

---

### `AdvancedCrawler`

#### –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä:

```python
AdvancedCrawler(config_path=None, cli_args=None)
```

* **config_path** ‚Äî –ø—É—Ç—å –∫ YAML –∫–æ–Ω—Ñ–∏–≥—É
* **cli_args** ‚Äî —Å–ª–æ–≤–∞—Ä—å —Å –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞—Å—Ç—Ä–æ–µ–∫

#### –ú–µ—Ç–æ–¥—ã:

* `run()` ‚Äî –∑–∞–ø—É—Å–∫–∞–µ—Ç –∫—Ä–∞—É–ª–µ—Ä, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
* –õ–æ–≥–∏—Ä—É–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å, –æ—à–∏–±–∫–∏, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É (JSON + HTML)

---

## üîπ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (YAML)

–ü—Ä–∏–º–µ—Ä `config.yaml`:

```yaml
start_urls:
  - "https://www.wikipedia.org/"
max_pages: 100
crawler:
  max_concurrent: 10
  max_depth: 2
  rate_limit: 5
  respect_robots: false
  include_patterns: []
  exclude_patterns: []
  allowed_domains: ["wikipedia.org"]
storage:
  type: "json"
  path: "results.json"
log_file: "crawler.log"
log_level: "INFO"
```

* `max_concurrent` ‚Äî —á–∏—Å–ª–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
* `max_depth` ‚Äî –≥–ª—É–±–∏–Ω–∞ –æ–±—Ö–æ–¥–∞ —Å—Å—ã–ª–æ–∫
* `rate_limit` ‚Äî –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
* `respect_robots` ‚Äî —É—á–∏—Ç—ã–≤–∞—Ç—å robots.txt
* `storage` ‚Äî –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

---

## üîπ –ë–µ–Ω—á–º–∞—Ä–∫

```bash
python src/benchmark/run_benchmark.py
```

* –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç **SYNC vs ASYNC** –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—é –ø–∞–º—è—Ç–∏
* –ú–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å –Ω–∞–≥—Ä—É–∑–∫—É —á–µ—Ä–µ–∑ `pages_list = [100, 500, 1000]`

---

## üîπ –õ–æ–≥–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

* –ü—Ä–æ–≥—Ä–µ—Å—Å –ª–æ–≥–∏—Ä—É–µ—Ç—Å—è –≤ –∫–æ–Ω—Å–æ–ª—å: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü, —Å–∫–æ—Ä–æ—Å—Ç—å, –æ—à–∏–±–∫–∏
* –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è:

  * `stats.json` ‚Äî –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ, —É—Å–ø–µ—à–Ω—ã–µ/–Ω–µ—É—Å–ø–µ—à–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, —Å—Ç–∞—Ç—É—Å-–∫–æ–¥—ã
  * `report.html` ‚Äî –≤–∏–∑—É–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç —Å –≥—Ä–∞—Ñ–∏–∫–∞–º–∏

---

## üîπ –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

* Python 3.11+
* aiohttp
* beautifulsoup4
* tqdm
* pyyaml (–¥–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏)
* sqlite3 (–¥–ª—è SQLite-—Ö—Ä–∞–Ω–∏–ª–∏—â–∞)

---

## üîπ –ê–≤—Ç–æ—Ä

–í–ª–∞–¥–∏–º–∏—Ä –ö–æ–º–∏—Å—Å–∞—Ä–æ–≤