# main.py
import asyncio
import json
from aiohttp import web
from crawler.async_crawler import AsyncCrawler
from crawler.errors import TransientError, PermanentError

# --- 1Ô∏è‚É£ –õ–æ–∫–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä —Å —Ä–∞–∑–Ω—ã–º–∏ —Å—Ç–∞—Ç—É—Å–∞–º–∏ ---
async def handler_200(request):
    return web.Response(text="‚úÖ OK", status=200)

async def handler_500(request):
    return web.Response(text="‚ö†Ô∏è Server Error", status=500)

async def handler_503(request):
    return web.Response(text="‚ö†Ô∏è Service Unavailable", status=503)

async def handler_404(request):
    return web.Response(text="‚ùå Not Found", status=404)

def create_test_server():
    app = web.Application()
    app.router.add_get("/200", handler_200)
    app.router.add_get("/500", handler_500)
    app.router.add_get("/503", handler_503)
    app.router.add_get("/404", handler_404)
    runner = web.AppRunner(app)
    return runner

# --- 2Ô∏è‚É£ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è main ---
async def main():
    # --- –°—Ç–∞—Ä—Ç—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä –Ω–∞ 8080 ---
    runner = create_test_server()
    await runner.setup()
    site = web.TCPSite(runner, "localhost", 8080)
    await site.start()
    print("üåê Test server running at http://localhost:8080")

    # --- URL –¥–ª—è —Ç–µ—Å—Ç–∞ ---
    test_urls = [
        "http://localhost:8080/200",  # —É—Å–ø–µ—Ö
        "http://localhost:8080/500",  # transient ‚Üí retry
        "http://localhost:8080/503",  # transient ‚Üí retry
        "http://localhost:8080/404",  # permanent ‚Üí no retry
    ]

    # --- –°–æ–∑–¥–∞—ë–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º crawler ---
    async with AsyncCrawler(
        max_concurrent=3,
        max_depth=1,
        respect_robots=False  # —á—Ç–æ–±—ã –ª–æ–∫–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä –Ω–µ –º–µ—à–∞–ª robots.txt
    ) as crawler:

        print("\nüöÄ Starting crawl...\n")
        results = await crawler.crawl(test_urls, max_pages=10)

        print("\n‚úÖ Crawl finished\n")

        # --- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ---
        print("üìä ===== Statistics =====")
        print("Processed URLs:", len(crawler.processed_urls))
        print("Failed URLs:", len(crawler.failed_urls))
        print("Errors by type:", crawler.stats["errors"])
        print("Successful retries:", crawler.stats["success_retries"])

        if crawler.stats["retry_times"]:
            avg_retry = sum(crawler.stats["retry_times"]) / len(crawler.stats["retry_times"])
        else:
            avg_retry = 0

        print(f"Average retry delay: {avg_retry:.2f}s")

        # --- –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á—ë—Ç ---
        report = {
            "processed_urls": list(crawler.processed_urls.keys()),
            "failed_urls": crawler.failed_urls,
            "error_stats": crawler.stats,
        }

        with open("crawler_report.json", "w", encoding="utf-8") as f:
            json.dump(report, f, indent=4, ensure_ascii=False)

        print("\nüìÑ Report saved to crawler_report.json")

    # --- –í—ã–∫–ª—é—á–∞–µ–º —Å–µ—Ä–≤–µ—Ä ---
    await runner.cleanup()
    print("üõë Test server stopped.")

# --- 3Ô∏è‚É£ –ó–∞–ø—É—Å–∫ ---
if __name__ == "__main__":
    asyncio.run(main())
